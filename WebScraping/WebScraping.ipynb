{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a60694c",
   "metadata": {},
   "source": [
    "# Data 200: Database Systems and Data Management for Data Analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9048bf2",
   "metadata": {},
   "source": [
    "# Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d999038",
   "metadata": {},
   "source": [
    "# Take Home Final Exam\n",
    "<font color='red'>**Due Date:** May 16, 12pm (noon) </font>\n",
    "---\n",
    "\n",
    "Task: Scrape data from a job search website\n",
    "\n",
    "\n",
    "Website: Indeed.com\n",
    "\n",
    "Objective: Collect job postings that match certain keywords and location filters, and then perform data analysis to extract insights about the job market.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Choose a location and a set of keywords that are relevant to your field of study. For example, you can schoose a field such as data science, computer science, or any other field you are interested in. In terms of the location, you might choose to search for \"data analytics\" jobs in \"San Francisco\". You are free to choose the field and location on this assignment.\n",
    "2. Scrape job postings from Indeed.com using selenium and python. Your code should extract the following information for each job posting:\n",
    "- Job title\n",
    "- Company name\n",
    "- Job description\n",
    "- Job location\n",
    "- Date posted\n",
    "3. Clean and preprocess the data as necessary. You may want to remove duplicates and perform other data cleaning tasks to prepare the data for analysis.\n",
    "4. Use data analysis techniques to extract insights about the job market. For example, you might:\n",
    "- Identify which companies are hiring the most for the given job titles and locations.\n",
    "- Determine the distribution of job titles and their average salaries.\n",
    "- Analyze the frequency of certain keywords in job descriptions, and determine which skills and qualifications are most in demand.\n",
    "5. Write a report summarizing your findings. Your report should include tables and visualizations to support your conclusions, and should provide actionable insights that could be used by job seekers, employers, or policymakers.\n",
    "Submit your code and report as a single .ipynb file (you can do it in this current notebook as a combination of code cells and markdown cells), along with any necessary instructions for running your code. Make sure your code is well-documented and organized, and that your report is well-written and easy to follow. <br> <br>\n",
    "Note: You do not need to perform text analysis or create word clouds for this exam. However, if you are interested in learning more about these techniques, you may want to explore them on your own as a side project.\n",
    "<br><br>\n",
    "Here is the rubric that I will use to grade your final exam:\n",
    "\n",
    "| Item                        | Weight |\n",
    "|-----------------------------|--------|\n",
    "| Code accuracy               | 25%    |\n",
    "| Code clarity and annotation | 25%    |\n",
    "| Exploratory data analysis   | 25%    |\n",
    "| Discussion of findings      | 25%    |\n",
    "\n",
    "<br>\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ecf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import presence_of_element_located\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the webdriver and opens a new window\n",
    "driver=webdriver.Chrome('/Users/caleb/Downloads/chromedriver_mac_arm64/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraps the data of the job descriptions, then clicks the next jobcard to do so for the next description\n",
    "def getRightPageDetails():\n",
    "    links=[]\n",
    "    descriptions=[]\n",
    "    \n",
    "    # finds the xpath's to the links which expand the card to view the full description\n",
    "    linkToRightPanel=driver.find_elements('xpath','//*[contains(@aria-label,\"full\")]')\n",
    "    \n",
    "    # adds each the links to a list\n",
    "    for i in linkToRightPanel:\n",
    "        links.append(i.get_attribute('id'))\n",
    "    \n",
    "    # iterates through the list to find the links for the descriptions then clicks the link to open the next description\n",
    "    for i in links:\n",
    "        button = driver.find_element('xpath','//*[@id=\"'+i+'\"]')\n",
    "        button.click()\n",
    "        time.sleep(random.uniform(2,3))\n",
    "        \n",
    "        # adds the full job description to a list\n",
    "        descriptions.append(getFullText())\n",
    "        \n",
    "    return descriptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraps the descriptions of the currently open job listing\n",
    "def getFullText():\n",
    "    # Due to slow loading times, the function does not run until the page loads and the job descriptions is found\n",
    "    WebDriverWait(driver, 20).until(presence_of_element_located((By.ID, \"jobDescriptionText\")))\n",
    "    \n",
    "    # scrpas the job description\n",
    "    fullJobDescription = driver.find_element('xpath', '//*[@id=\"jobDescriptionText\"]')\n",
    "    return fullJobDescription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraps the data of all the items which can be found in the job cards on the left side of the page\n",
    "def getLeftPageDetails():\n",
    "    jobDetails=[]\n",
    "    \n",
    "    for i in range(1,19):\n",
    "        # Due to the websites layout job cards with the values 6,12, and 18 are blank\n",
    "        if i not in (6,12,18):\n",
    "            \n",
    "            # Scraps the data of the job title\n",
    "            jobTitleElement=driver.find_element('xpath','//*[@id=\"mosaic-provider-jobcards\"]/ul/li['+str(i)+']/div/div[1]/div/div[1]/div/table[1]/tbody/tr/td/div[1]/h2')\n",
    "            jobTitle=jobTitleElement.text\n",
    "            \n",
    "            # Scraps the data of the company name\n",
    "            companyNameElement=driver.find_element('xpath','//*[@id=\"mosaic-provider-jobcards\"]/ul/li['+str(i)+']/div/div[1]/div/div[1]/div/table[1]/tbody/tr/td/div[2]/span[1]')\n",
    "            companyName=companyNameElement.text\n",
    "            \n",
    "            # Scraps the data of the jobs location\n",
    "            locationElement=driver.find_element('xpath','//*[@id=\"mosaic-provider-jobcards\"]/ul/li['+str(i)+']/div/div[1]/div/div[1]/div/table[1]/tbody/tr/td/div[2]/div')\n",
    "            location=locationElement.text\n",
    "            \n",
    "            # Scraps the data of the date posted\n",
    "            postedElement=driver.find_element('xpath','//*[@id=\"mosaic-provider-jobcards\"]/ul/li['+str(i)+']/div/div[1]/div/div[1]/div/table[2]/tbody/tr[2]/td/div/span')\n",
    "            posted=postedElement.text\n",
    "\n",
    "            # Adds all the details to a list of lists\n",
    "            jobDetails.append([jobTitle,companyName,location,posted])\n",
    "    return jobDetails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613025f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllData():\n",
    "    \n",
    "    #loads the start page of the data\n",
    "    driver.get('https://www.indeed.com/jobs?q=computer+science&l=New+York%2C+NY&radius=100&start=790&pp=gQShAAABh_3LvhkAAAACApK44gJqAQcBh5ACFTQgQg1ECtIBB-4BBgrchZ5mE9F36zPAFFiSnHunGxZgu-dZyF6zciqWHUZ_ObbZa03LDY5-J98VUuq-VKarE6BeXS4Pid2h84GmEILqsV0LO59FDCyOiboR12lItimq4s3jP13smhq2bcOxGsmyPxORJ_dCHPAlzQiQ4rDoq-GdW1Fl2S5CYO2UbTIg4fb2IM1E7BQZ03JxTPdEaneYCAW8mVmVPErAQP33JKvIga2lZbucz78cYrbwmqE_9E8fl201z_Q0kD1hRgZTd2Pphu8Gnzs1ruOBWGCOJmbb-91mDa29hc2I0KOgNyIpdBxQt4UrzmhrBBghyaT7A845zm5QiPl7u7c7J__Gkkg7I1uosXqY_Qptvk5EzUHb-wrSNGSEIlVtXMln9cyHDsh8KoBCJzLNnRms9W7V8r1GLz8k80GGvz7j32k_x0smUSNv5iC1pb3n66cr13HqiEbDu-j-mbxXBlcX0xsVaERpMbx5wFWOl29GLRy3_ZB8SFfVnEO0YxdX_Is2C_SAK2KekEjy6L42OV9VifYb9_xGbMG2msh9C4gUQj_www6JoVKQpNeKF45NaNEh5eljhhY90ziKcAFlUiCDNg35AbBADrjt2PS6qTm5VG1HGQrG8NjnRz-exYB7_-VVWfPVZWN5QdLZEiM_urHaBlXb_q8tBj_32TRasww3QXc0ESLwCSBgDgHuw4jFr1KoC6LSqpFa8ESE-M_6QpDoc3rEOGq5nDwoyLlj6Df93H3zVUNSCH_mi_KV2hNfAf5vB643Kxp3o_eOtgRWTqoROlWakqFOU5tWva2-AAA&vjk=96d6f66d3e0297d4')\n",
    "    \n",
    "    #creates an empty data frame which will be filled with the page contents\n",
    "    totalDetails = pd.DataFrame()\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        # The try except block runs the method and throws an error message if it fails\n",
    "        try:\n",
    "            \n",
    "            # The driver waits till the job description is fully loaded to ensure the program doesnt crash\n",
    "            WebDriverWait(driver, 20).until(presence_of_element_located((By.ID, \"jobDescriptionText\")))\n",
    "            \n",
    "            # scraping data from the left side of a single page\n",
    "            pageDetailsL = getLeftPageDetails()\n",
    "\n",
    "            # scraping data from the right side of a single page\n",
    "            pageDetailsR = getRightPageDetails()\n",
    "\n",
    "            # df with the current page data, left side\n",
    "            singleLeftDetails = pd.DataFrame(pageDetailsL,columns=['Job Title','Company','Location','Date Posted'])\n",
    "\n",
    "            # df with the current page data, right side\n",
    "            singleRightDetails = pd.DataFrame(pageDetailsR,columns=['Comment'])\n",
    "            \n",
    "            # Joins the data of the left side details and right side details\n",
    "            singleDetails=singleLeftDetails.join(singleRightDetails,how='outer')\n",
    "\n",
    "            # Adds each page the entire data\n",
    "            totalDetails = pd.concat([totalDetails,singleDetails])\n",
    "\n",
    "            # clicks on a the next page\n",
    "            # The try except block is present to prevent the program from clicking next page on the final page\n",
    "            # to ensure that the program doesn't crash\n",
    "            try:            \n",
    "                button = driver.find_element('xpath','//*[contains(@aria-label,\"Next Page\")]')\n",
    "                button.click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        except:\n",
    "            print('Error!')\n",
    "        \n",
    "        # Due to the slow run time of the program the data is converted to a csv file and downloaded to avoid having to\n",
    "        # rerun the entire program.\n",
    "        totalDetails.to_csv('details0-100.csv')\n",
    "        return totalDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36cb352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans the text of a given string\n",
    "def cleanText(text):\n",
    "    # ensures that the given object is a string to avoid throwing an error\n",
    "    if type(text)==str:\n",
    "            \n",
    "        # Remove leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "\n",
    "        # Remove new lines character (\\n)\n",
    "        text = text.replace('\\n',' ')\n",
    "\n",
    "        # Remove extra whitespaces\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa98fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing and cleaning the data\n",
    "\n",
    "# Reads the data from the downloaded csv file with all the information - \n",
    "df = pd.read_csv('/Users/caleb/Desktop/GitDownloads/final-exam-calebloring/exams/final/details0-100.csv')\n",
    "\n",
    "# Removes the duplicate listings from the dataframe\n",
    "df=df.drop_duplicates(subset='Comment')\n",
    "\n",
    "# Cleaning the text of each column (which needs cleaning)\n",
    "df['Location']=df['Location'].apply(cleanText)\n",
    "df['Date Posted']=df['Date Posted'].apply(cleanText)\n",
    "df['Comment']=df['Comment'].apply(cleanText)\n",
    "\n",
    "# Remove listings with missing information\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Resets the index following the preprocessing\n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates data frame where each job is remote work\n",
    "df_remote = df[df['Location'].str.contains('remote',case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3bd57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates data frame where each job requires a bachelors degree\n",
    "df_education_bs = df[df['Comment'].str.contains('bachelor|bachelors',regex=True,case=False)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1636037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates data frame where each job requires a masters degree\n",
    "df_education_ms = df[df['Comment'].str.contains('masters',regex=True,case=False)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb106f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates data frame where each job had been posted at least 30+ days before the data was scrapped\n",
    "df_posted = df[df['Date Posted'].str.contains('30+',case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates data frame where each job had been posted the day before or the day of when the data was scrapped\n",
    "df_posted_recent = df[df['Date Posted'].str.contains('(Posted 1 day|just posted)',regex=True,case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates data frame where every lisitng requuires SQL or pandas experience\n",
    "df_skills = df[df['Comment'].str.contains('SQL|pandas',regex=True,case=False)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68643365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates variables which satisfy each condition and are also remote\n",
    "df_postedrecent_remote = pd.merge(df_posted_recent,df_remote).drop_duplicates(subset='Comment')\n",
    "df_olderlisting_remote = pd.merge(df_posted,df_remote).drop_duplicates(subset='Comment')\n",
    "\n",
    "df_education_bs_remote = pd.merge(df_education_bs,df_remote).drop_duplicates(subset='Comment')\n",
    "df_education_ms_remote = pd.merge(df_education_ms,df_remote).drop_duplicates(subset='Comment')\n",
    "\n",
    "df_skills_remote = pd.merge(df_skills,df_remote).drop_duplicates(subset='Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab929b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates variable containing the percent of variables that are also remote\n",
    "\n",
    "df_postedrecent_remote_percent = round((len(df_postedrecent_remote)/len(df_posted_recent))*100,2)\n",
    "df_olderlisting_remote_percent = round((len(df_olderlisting_remote)/len(df_posted))*100,2)\n",
    "df_education_bs_remote_percent = round((len(df_education_bs_remote)/len(df_education_bs))*100,2)\n",
    "df_education_ms_remote_percent = round((len(df_education_ms_remote)/len(df_education_ms))*100,2)\n",
    "df_skills_remote_percent = round((len(df_skills_remote)/len(df_skills))*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e1f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns for final dataframe\n",
    "\n",
    "num_in_table=[len(df_remote),len(df_posted_recent),len(df_posted),len(df_education_bs),\n",
    "              len(df_education_ms),len(df_skills)]\n",
    "\n",
    "num_remote=[len(df_remote),len(df_postedrecent_remote),len(df_olderlisting_remote),len(df_education_bs_remote),\n",
    "            len(df_education_ms_remote),len(df_skills_remote)]\n",
    "\n",
    "num_remote_percent=[100,df_postedrecent_remote_percent,df_olderlisting_remote_percent,\n",
    "                    df_education_bs_remote_percent,df_education_ms_remote_percent,df_skills_remote_percent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index for final dataframe \n",
    "row_names=['Remote','Day old or Posted Today','Posted 30+ Days Ago','Bachelors required',\n",
    "           'Masters Required','SQL/pandas required']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe to display values\n",
    "final_df=pd.DataFrame(index=row_names)\n",
    "final_df['Number of Jobs']=num_in_table\n",
    "final_df['Number of jobs which are remote']=num_remote\n",
    "final_df['Percent of jobs which are remote']=num_remote_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create row names and percent values\n",
    "bars = row_names\n",
    "percents = num_remote_percent\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# Plot the bar graph\n",
    "ax.bar(bars, percents)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Percetage of Jobs in each Variable which are Remote')\n",
    "ax.set_xlabel('Variables')\n",
    "ax.set_ylabel('Percents (%)')\n",
    "ax.set_yticks(np.arange(0,105,5))\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecdd1c",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Using data from 1500 computer science jobs based in New York, NY posted to indeed.com, insights have been made about the job market. The following data was collected from each job listing: the job title, company name, location, date posted, and the full job description. This data provided thorough information to conduct a data analysis of the job market.\n",
    "\n",
    "Following the COVID-19 pandemic many tech-based jobs, especially those in cities with high rents such as New York, have been hiring and increasing number of remote workers. The data analysis performed attempted to determine which elements of the job postings influenced whether a job was remote.   \n",
    "\n",
    "The data was cleaned immediately after being scraped. This clean up included removing excess white spaces, new line characters, and job listings with missing data. Duplicate job listings were then removed. The data was compiled into a pandas data frame ready to be analyzed. The first variable examined was to see when jobs were posted. Two data frames were created, job listings which had been posted on the day the data was scraped or the day before, and a second table which included all the listings which were posted more than 30 days ago. This was to see how those posted more recently compared to jobs which were struggling to be filled. The next variable was education, two more data frames were constructed. One in which a bachelor’s degree was required in the full description and the other where a master’s degree was required. This was to determine whether more advanced jobs were more likely to be remote. The final variable checked created a data frame based on the skills required, namely which descriptions asked for experience with SQL or pandas. This was to determine if jobs which require certain skills are more likely to be remote.\n",
    "\t\n",
    "The data frames where then all merged with a data frame which contained all the jobs which are remote. These tables provided information on how many of the jobs with each condition are also remote work. The number of jobs which satisfied each variable, the number of these which were also remote, and the percent which were remote where put into a table which can be found below (Table 1). Since each condition had varying amounts of jobs, the percent of jobs which were also remote is a more reliable statistic on how the variable influences likelihood to be remote. The precents of jobs which satisfied each condition and where remote were graphed and can be found in the graph below (Chart 1).\n",
    "\n",
    "# Conclusion:\n",
    "\n",
    "The results of the data analysis are shocking. Surprising all of the variables had a nearly identical percent of jobs being remote. All the precents are within 5% of each other, they ranged from roughly 25-30%. On first thought these seems to be logical as 331 of the 1265 of the total jobs are remote, which amounts to 26.17%. However, each separate condition checked was chosen for specific reasons which were believed to have an effect on likelihood a job would be remote. Jobs posted recently may have been more likely to be remote as it is easier to hire and onboard an employee working remote. Job listings which were posted a month ago were thought to be less likely to be remote as applying to in person jobs may require moving which deters some potential applicants. Education level was analyzed because it seemed as though jobs which require more education, are more advanced jobs, which would increase the chances of in-person work being required. The final variable checked if the skills required were affected by a jobs remoteness because it was assumed that jobs which require more advanced skills would be more likely to not be remote. However, none of these conditions had any effect on whether a job would be remote. This could be due to a multitude of reasons. One is that computer science jobs are often worked individually and online. These means that no matter the difficulty of work, working remotely flows with relative ease. Another factor as previously mentioned, is COVID and high rent. When many employees were forced to work remote over the pandemic employers noticed that many of these jobs can be done remotely. When based in a city like New York, companies determined it is not worth it to spend money on expensive office buildings when employees, who are happy to do so, can work from home. This data analysis concludes that in a tech-based and mostly individually work centered industry, work may be remote regardless of how long the job listing was posted for, level of education required, or the skills required.\n",
    "\n",
    "**Table 1**\n",
    "\n",
    "![image not found](Table_1.png)\n",
    "\n",
    "**Chart 1**\n",
    "![image not found](Chart_1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
